<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Syllabus for the Datamining Class</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Syllabus for the Datamining Class" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Syllabus for the Datamining Class" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Gener Avilés R">


<meta name="date" content="2017-04-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="canonical-correlation-analysis-cca.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Mining Syllabus</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-does-pca-do"><i class="fa fa-check"></i><b>2.1</b> What does PCA do?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#pca-step-by-step"><i class="fa fa-check"></i><b>2.2</b> PCA Step by Step</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#getting-the-dataset-and-things-ready."><i class="fa fa-check"></i><b>2.2.1</b> 1. Getting the dataset and things ready.</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#centering-the-points"><i class="fa fa-check"></i><b>2.2.2</b> 2. Centering the points</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#compute-covariance-sigma_xy-matrix"><i class="fa fa-check"></i><b>2.2.3</b> 3. Compute covariance (<span class="math inline">\(\sigma_{X,Y}\)</span>) matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="intro.html"><a href="intro.html#eigenvectors-eigenvalues"><i class="fa fa-check"></i><b>2.2.4</b> 4. Eigenvectors + Eigenvalues</a></li>
<li class="chapter" data-level="2.2.5" data-path="intro.html"><a href="intro.html#pick-md-eigenvectors-with-highest-eigenvalues."><i class="fa fa-check"></i><b>2.2.5</b> 5. Pick <span class="math inline">\(m&lt;d\)</span> eigenvectors with highest eigenvalues.</a></li>
<li class="chapter" data-level="2.2.6" data-path="intro.html"><a href="intro.html#project-datapoints-to-those-eigenvectors."><i class="fa fa-check"></i><b>2.2.6</b> 6. Project datapoints to those eigenvectors.</a></li>
<li class="chapter" data-level="2.2.7" data-path="intro.html"><a href="intro.html#perform-analysis-as-needed-according-to-study."><i class="fa fa-check"></i><b>2.2.7</b> 7. Perform analysis as needed according to study.</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#pros-and-cons-of-pca"><i class="fa fa-check"></i><b>2.3</b> Pros and Cons of PCA</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>3</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#what-is-cca"><i class="fa fa-check"></i><b>3.1</b> What is CCA?</a><ul>
<li class="chapter" data-level="3.1.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-r-r_c"><i class="fa fa-check"></i><b>3.1.1</b> Canonical R (<strong><span class="math inline">\(R_c\)</span></strong>) <br \></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#assumptions-for-cca"><i class="fa fa-check"></i><b>3.2</b> Assumptions for CCA</a></li>
<li class="chapter" data-level="3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#objectives-of-cca"><i class="fa fa-check"></i><b>3.3</b> Objectives of CCA</a></li>
<li class="chapter" data-level="3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#terms-used-in-the-context-of-a-cca-analysis"><i class="fa fa-check"></i><b>3.4</b> Terms used in the context of a CCA analysis</a></li>
<li class="chapter" data-level="3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#interpreting-canonical-variates"><i class="fa fa-check"></i><b>3.5</b> Interpreting canonical variates</a></li>
<li class="chapter" data-level="3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#considerations-when-working-with-cca"><i class="fa fa-check"></i><b>3.6</b> Considerations when working with CCA</a></li>
<li class="chapter" data-level="3.7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#limitations-of-cca"><i class="fa fa-check"></i><b>3.7</b> Limitations of CCA</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html"><i class="fa fa-check"></i><b>4</b> Self Organizing Maps (SOM)</a><ul>
<li class="chapter" data-level="4.1" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#other-names"><i class="fa fa-check"></i><b>4.1</b> Other names:</a></li>
<li class="chapter" data-level="4.2" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#generalities"><i class="fa fa-check"></i><b>4.2</b> Generalities</a></li>
<li class="chapter" data-level="4.3" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#penfields-homunculus"><i class="fa fa-check"></i><b>4.3</b> Penfield´s Homunculus</a></li>
<li class="chapter" data-level="4.4" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#algorithm"><i class="fa fa-check"></i><b>4.4</b> Algorithm</a><ul>
<li class="chapter" data-level="4.4.1" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#conditions"><i class="fa fa-check"></i><b>4.4.1</b> Conditions</a></li>
<li class="chapter" data-level="4.4.2" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#steps-of-the-algorithm"><i class="fa fa-check"></i><b>4.4.2</b> Steps of the algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#example"><i class="fa fa-check"></i><b>4.5</b> Example</a><ul>
<li class="chapter" data-level="4.5.1" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#summary-of-the-dataset"><i class="fa fa-check"></i><b>4.5.1</b> Summary of the dataset</a></li>
<li class="chapter" data-level="4.5.2" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#training-phase"><i class="fa fa-check"></i><b>4.5.2</b> Training Phase</a></li>
<li class="chapter" data-level="4.5.3" data-path="self-organizing-maps-som.html"><a href="self-organizing-maps-som.html#visualization"><i class="fa fa-check"></i><b>4.5.3</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Syllabus for the Datamining Class</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="self-organizing-maps-som" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Self Organizing Maps (SOM)</h1>
<div id="other-names" class="section level2">
<h2><span class="header-section-number">4.1</span> Other names:</h2>
<ul>
<li>Self-Organizing Feature Map (SOFM).</li>
<li>Kohonen Map.</li>
<li>Kohonen Networks.</li>
</ul>
</div>
<div id="generalities" class="section level2">
<h2><span class="header-section-number">4.2</span> Generalities</h2>
<ul>
<li><strong>S</strong>elf <strong>O</strong>rganizing <strong>M</strong>aps belong to the family of <strong>Artificial Neural Networks</strong>.</li>
<li>In the subgroup of <strong>Unsupervised Learning</strong>,</li>
<li>To function they use a <strong>competitive learning strategy</strong> (winner takes all).</li>
<li>They are considered to be a <strong>non-linear</strong> implementation of the <em>Principal Components Analysis</em> (PCA) algorithm. <br \></li>
</ul>
<p>Self Organizing Maps (SOM) were first described by <a href="http://www.cis.hut.fi/research/som-research/teuvo.html">Teuvo Kohonen</a> <span class="citation">(Kohonen <a href="#ref-kohonen1995self">1995</a>)</span>, others have extended his work and modified SOMs to tackle specific problems.</p>
<p>“The Self-Organizing Map is inspired by postulated feature maps of neurons in the brain comprised of feature-sensitive cells that provide ordered projections between neuronal layers, such as those that may exist in the retina and cochlea. For example, there are acoustic feature maps that respond to sounds to which an animal is most frequently exposed, and tonotopic maps that may be responsible for the order preservation of acoustic resonances.” <span class="citation">(Brownlee <a href="#ref-brownlee2011clever">2011</a>)</span> <br \><br \> Different sensory inputs are maped into corresponding areas of the cerebral cortex in an orderly way. The <em>map</em> generated in the cerebral cortex is called a <strong>topographic map</strong> and it has two very important properties, <span class="citation">(Bullinaria <a href="#ref-somFundamentals">2015</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>At each stage of representation, or processing, each piece of incoming information is kept in its proper context/neighborhood.</li>
<li>Neurons dealing with closely related pieces of information are kept close together so that they can interact via short synaptic connections.</li>
</ol>
<p><br \></p>
<p>Following the principles observed in the sensory input processing by neurological structures , the previous two properties should be kept in an artificial intelligence algorithm looking to reproduce this phenomenon. In shorter words: the principle of topographic map formation is the escence of this process, where: <br \></p>
<blockquote>
<p>“The spatial location of an output neuron in a topographic map corresponds to a particular domain or feature drawn from the input space.” <span class="citation">(Bullinaria <a href="#ref-somFundamentals">2015</a>)</span></p>
</blockquote>
</div>
<div id="penfields-homunculus" class="section level2">
<h2><span class="header-section-number">4.3</span> Penfield´s Homunculus</h2>
</div>
<div id="algorithm" class="section level2">
<h2><span class="header-section-number">4.4</span> Algorithm</h2>
<div id="conditions" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Conditions</h3>
<ul>
<li>The data given to the algorithm must be continuous.</li>
<li>The algorithm will perform better when fed high dimensional data.</li>
<li>This algorithm will help in the process of</li>
</ul>
</div>
<div id="steps-of-the-algorithm" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Steps of the algorithm</h3>
<p><em>This section is based on the “Introduction to Neural Computation” course, taught by <a href="http://www.cs.bham.ac.uk/~jxb/">Dr John A Bullinaria</a> at the <a href="http://www.cs.bham.ac.uk">University of Birmingham</a>.</em></p>
<p>Self Organizing Maps follow the principle of <strong>self organization</strong>, composed of the following 4 steps:</p>
<ol style="list-style-type: decimal">
<li>Initialization.</li>
<li>Competition.</li>
<li>Cooperation.</li>
<li>Adaptation.</li>
</ol>
<div id="initialization" class="section level4">
<h4><span class="header-section-number">4.4.2.1</span> Initialization</h4>
<p>All the connections weights (neurons or centroids) are initialized with small random values. Other authors use the the principal component (eigenvector) of the data in this step.</p>
</div>
<div id="competition" class="section level4">
<h4><span class="header-section-number">4.4.2.2</span> Competition:</h4>
<p>For each input pattern, the neurons compute their respective values of a <em>discriminant function</em> which provides the basis for competition. The particular neuron with the smallest value of the discriminant function is declared the winner.<br \></p>
<ul>
<li>Given the input space is <span class="math inline">\(D\)</span> dimensional, the input patterns can be expressed as:</li>
</ul>
<p><span class="math display">\[x = \{x_i:i=1,...,D\}\]</span></p>
<ul>
<li>The connection weights between the input units <span class="math inline">\(i\)</span> and the neurons <span class="math inline">\(j\)</span> in the computation layer can be expressed as:</li>
</ul>
<p><span class="math display">\[w_j = \{w_{ji}: j = 1,...,N; i = 1,...,D\}\]</span></p>
<pre><code> Where N is the total number of neurons.</code></pre>
<p><strong>Discriminant Function</strong><br\></p>
<p>Now that the dimensions of the data and how they are related to the lattice in the SOM are clearer, the next natural step in the process is to define the <em>discriminant function</em>, SOM bases this on the squared <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>:</p>
<p><span class="math display">\[d_j(x) = \sum_{i=1}^D(x_i - w_{ji})^2\]</span></p>
<p>By doing this, the neuron whose weight vector comes closest to the input vector (most similar to it) is “declared” the winner.</p>
<p>This process allows us to <strong>map a continuous space given by the input to a discrete output space of neurons</strong> by a process of competition and a “winner takes all” approach.</p>
</div>
<div id="cooperation" class="section level4">
<h4><span class="header-section-number">4.4.2.3</span> Cooperation:</h4>
<p>The winning neuron determines the spatial location of a topological neighborhood of excited neurons, thereby providing the basis for cooperation among neighbouring neurons. This <strong>lateral interaction</strong> within a set of neurons has been observed by neurobiologist in human and other brains. When one neuron fires, its closest neighbours tend to get excited more than those further away. This phenomenon allows the formation of a <strong>topological neighborhood</strong> that decays with distance in relation to the excited neuron known as the <strong>Best Matching Unit (BMU)</strong> in a SOM. <br \></p>
<p>If <span class="math inline">\(S_ij\)</span> is the lateral distance between neurons <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> on the grid of output neurons, the topological neighborhood will then be expressed as:</p>
<p><span class="math display">\[T_{ij,I(x)} = exp \left( \frac{-S^2_{j,I(x)}}{2\sigma^2}\right)\]</span></p>
<p>Where <span class="math inline">\(I(x)\)</span> = the index of the winning neuron.</p>
<p>This has the following key properties:</p>
<ul>
<li>Maximal at the BMU.</li>
<li>It decreases monotonically to zero as the distance goes to infinity.</li>
<li>It is translation invariant, therefore independent of the location of the BMU.</li>
</ul>
<p><strong>Important note:</strong> <br \></p>
<p>The process of self organization will work best if the size of <span class="math inline">\(\sigma\)</span> of the neighborhood decreases with time. A popular approach has been an exponential decay:</p>
<p><span class="math display">\[\sigma(t) = \sigma_0\space exp(\frac{-t}{\tau_{\sigma}})\]</span></p>
</div>
<div id="adaptation" class="section level4">
<h4><span class="header-section-number">4.4.2.4</span> Adaptation:</h4>
<p>The excited neurons decrease their individual values of the discriminant function in relation to the input pattern through suitable adjustment of the associated connection weights, such that the response of the winning neuron to the subsequent application of a similiar input pattern is enhanced.</p>
<p>In a topographic neighborhood not only the winning neuron gets its weights updated, but its neighbors will have their weights updated as well, although by not as much as the winner itself. In practice, the appropriate weight update equation is:</p>
<p><span class="math display">\[\Delta w_{ji} = \eta(t) * T_{j,I(x)}(t) * (x_i - w_{ji})\]</span></p>
<p>Where there is a time (<span class="math inline">\(t\)</span>, epoch) dependent learning rate <span class="math inline">\(\eta(t) = \eta_0 \space exp(\frac{-t}{\tau_\eta})\)</span>, and the updates are applied for all the training patterns <span class="math inline">\(x\)</span> over many epochs.</p>
<p>Each learning weight update will move the weight vectors <strong><span class="math inline">\(w_i\)</span></strong> of the BMU (winning neuron) and it’s neighbors towards the input vector <strong><span class="math inline">\(x\)</span></strong>. Repeated presentations of the training data thus leads to topological ordering.</p>
</div>
</div>
</div>
<div id="example" class="section level2">
<h2><span class="header-section-number">4.5</span> Example</h2>
<p>This example is based on material presented in Dublin for the 2014 R Regional Conference <span class="citation">(Lynn <a href="#ref-shane">2014</a>)</span> <br \> We will take the dataset <code>wines</code> found in the package <code>kohonen</code> <span class="citation">(Ron Wehrens <a href="#ref-R-kohonen">2017</a>)</span>, which shows the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.</p>
<div id="summary-of-the-dataset" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Summary of the dataset</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(kohonen)
<span class="kw">data</span>(wines)
<span class="kw">summary</span>(wines)</code></pre></div>
<pre><code>##     alcohol        malic acid        ash        ash alkalinity 
##  Min.   :11.03   Min.   :0.74   Min.   :1.360   Min.   :10.60  
##  1st Qu.:12.36   1st Qu.:1.60   1st Qu.:2.210   1st Qu.:17.20  
##  Median :13.05   Median :1.87   Median :2.360   Median :19.50  
##  Mean   :12.99   Mean   :2.34   Mean   :2.366   Mean   :19.52  
##  3rd Qu.:13.67   3rd Qu.:3.10   3rd Qu.:2.560   3rd Qu.:21.50  
##  Max.   :14.83   Max.   :5.80   Max.   :3.230   Max.   :30.00  
##    magnesium       tot. phenols     flavonoids    non-flav. phenols
##  Min.   : 70.00   Min.   :0.980   Min.   :0.340   Min.   :0.1300   
##  1st Qu.: 88.00   1st Qu.:1.740   1st Qu.:1.200   1st Qu.:0.2700   
##  Median : 98.00   Median :2.350   Median :2.130   Median :0.3400   
##  Mean   : 99.59   Mean   :2.292   Mean   :2.023   Mean   :0.3623   
##  3rd Qu.:107.00   3rd Qu.:2.800   3rd Qu.:2.860   3rd Qu.:0.4400   
##  Max.   :162.00   Max.   :3.880   Max.   :5.080   Max.   :0.6600   
##     proanth        col. int.         col. hue        OD ratio    
##  Min.   :0.410   Min.   : 1.280   Min.   :0.480   Min.   :1.270  
##  1st Qu.:1.250   1st Qu.: 3.210   1st Qu.:0.780   1st Qu.:1.930  
##  Median :1.550   Median : 4.680   Median :0.960   Median :2.780  
##  Mean   :1.587   Mean   : 5.055   Mean   :0.957   Mean   :2.604  
##  3rd Qu.:1.950   3rd Qu.: 6.200   3rd Qu.:1.120   3rd Qu.:3.170  
##  Max.   :3.580   Max.   :13.000   Max.   :1.710   Max.   :4.000  
##     proline      
##  Min.   : 278.0  
##  1st Qu.: 500.0  
##  Median : 672.0  
##  Mean   : 745.1  
##  3rd Qu.: 985.0  
##  Max.   :1680.0</code></pre>
</div>
<div id="training-phase" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Training Phase</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(kohonen)
<span class="co">#SOM grid/lattice</span>
som_grid &lt;-<span class="st"> </span><span class="kw">somgrid</span>(<span class="dt">xdim =</span> <span class="dv">6</span>, <span class="dt">ydim =</span> <span class="dv">6</span>, <span class="dt">topo =</span> <span class="st">&quot;hexagonal&quot;</span>)

<span class="co">#Standardazing dataset</span>
wines.sc &lt;-<span class="st"> </span><span class="kw">scale</span>(wines)

<span class="co">#Setting the seed</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co">#Training SOM</span>
som_model &lt;-<span class="st"> </span><span class="kw">som</span>(wines.sc,
                 <span class="dt">grid =</span> som_grid,
                 <span class="dt">rlen =</span> <span class="dv">15000</span>,
                 <span class="dt">alpha =</span> <span class="kw">c</span>(<span class="fl">0.05</span>,<span class="fl">0.01</span>),
                 <span class="dt">keep.data =</span> <span class="ot">TRUE</span>)</code></pre></div>
</div>
<div id="visualization" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Visualization</h3>
<div id="training-progress" class="section level4">
<h4><span class="header-section-number">4.5.3.1</span> Training progress</h4>
<p>Once a stable plateu is shown in the plot, one can assume that the Euclidean distance from each node’s weights to the samples in the database represented by that node (or neuron) has reached it’s lower value. If the curve in the plot is continually decreasing more iterations will be needed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(som_model, <span class="dt">type =</span> <span class="st">&quot;changes&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="node-counts" class="section level4">
<h4><span class="header-section-number">4.5.3.2</span> Node Counts</h4>
<p>This steps helps us visualize how many samples are mapped to each neuron on the map. This can be used as an indicator of <strong>map quality</strong>. Some key points:</p>
<ul>
<li>Sample distribution should be relatively uniform (colors or shades similar) throughout the map.</li>
<li>Large values in a map area are a sign that a larger map is needed.</li>
<li>Empty neurons are a sign that the map is to large.</li>
<li>It is costumary to aim for 5 to 10 samples per neuron for an ideal map.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&#39;coolBlueHotRed.R&#39;</span>)
<span class="kw">plot</span>(som_model,
     <span class="dt">type =</span> <span class="st">&quot;count&quot;</span>,
     <span class="dt">palette.name =</span> coolBlueHotRed,
     <span class="dt">main =</span> <span class="st">&quot;Counts plot / Map quality&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="neighbor-distance" class="section level4">
<h4><span class="header-section-number">4.5.3.3</span> Neighbor Distance</h4>
<p>This is also called the <strong><em>U-Matrix</em></strong> and it is a visualization of the distance between each neuron and it’s neihgbors. <br \> Key points:</p>
<ul>
<li>It’s usually visualized using a grayscale.</li>
<li>Areas of low neighbor distance indicate groups of similar neurons.</li>
<li>Areas of high nighbor distance indicate that neurons are more dissimilar.</li>
<li>The U-Matrix can be used to identify clusters.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(som_model,
     <span class="dt">type =</span> <span class="st">&quot;dist.neighbours&quot;</span>,
     <span class="dt">palette.name =</span> gray.colors,
     <span class="dt">main =</span> <span class="st">&quot;Nighbour distance plot/ U-Matrix&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="weight-vectors-codes" class="section level4">
<h4><span class="header-section-number">4.5.3.4</span> Weight vectors / Codes</h4>
<p>These codes are made up of normalised values of the original variables from the dataset used to train the SOM. Each node weight vector is similar (representative) of the samples mapped to that node. By doing this visualization it is possible to see patterns in the distribution of the values of variables. <br \></p>
<p>Key points:</p>
<ul>
<li>The visualization of &gt;7 variables with this approach is usually of not much use.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(som_model,
     <span class="dt">type =</span> <span class="st">&quot;codes&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Codes&quot;</span>,
     <span class="dt">palette.name =</span> coolBlueHotRed)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="heatmaps" class="section level4">
<h4><span class="header-section-number">4.5.3.5</span> Heatmaps</h4>
<p>One of the most important visualization of a SOM. It allows for a weight space view of a single variable (distribution). Usually, multiple heatmaps are produced based on different variables and then compared to see if areas and/or patterns of interest are discovered. Key points:</p>
<ul>
<li>When generating multiple visualizations of the same SOM, individual sample positions do not move form one visualization to another, the map is simpleyu colored by different variables.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">codes &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">as.list</span>(som_model$codes))

<span class="co">#Standardized values</span>
<span class="kw">plot</span>(som_model,
     <span class="dt">type =</span> <span class="st">&quot;property&quot;</span>,
     <span class="dt">property =</span> codes$alcohol,
     <span class="dt">palette.name =</span> coolBlueHotRed,
     <span class="dt">main =</span> <span class="st">&quot;Standardized Alcohol Levels&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Heat maps are usually presented to non-technicall people, therefore it is prefered to use the normal values as they appeared on the original dataset. The following heatmaps are presented with values previous to standardization.</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="figure"><span id="fig:unnamed-chunk-9"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-9-1.png" alt="OD Ratio in Wines" width="672" />
<p class="caption">
Figure 4.1: OD Ratio in Wines
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-10"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-10-1.png" alt="Magnesium levels in Wines" width="672" />
<p class="caption">
Figure 4.2: Magnesium levels in Wines
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-11"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-11-1.png" alt="Total Phenols levels in Wines" width="672" />
<p class="caption">
Figure 4.3: Total Phenols levels in Wines
</p>
</div>
</div>
<div id="drawing-conclusions" class="section level4">
<h4><span class="header-section-number">4.5.3.6</span> Drawing Conclusions</h4>
<p>Understanding how a Self Organizing Map is generated is of key value to run these analysis. Once a clear understanding of the mathematics and algorithmia is achieved then the process of training the SOM can start.<br \> Special attention must be put in the quality of the SOM, the right amount of ephocs and the size of the grid as well.</p>
<p>In the previous example one can see that wines with high levels of alcohol tend to have low OD ratios and high levels of total phenols. Combined with domain knowledge these assosiations can be put to good use in the field.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kohonen1995self">
<p>Kohonen, Teuvo. 1995. “Self-Organizing Maps, Volume 30 of Springer Series in Information Sciences.” Springer, Berlin, Heidelberg.</p>
</div>
<div id="ref-brownlee2011clever">
<p>Brownlee, Jason. 2011. <em>Clever Algorithms: Nature-Inspired Programming Recipes</em>. Jason Brownlee.</p>
</div>
<div id="ref-somFundamentals">
<p>Bullinaria, John A. 2015. “Self Organizing Maps: Fundamentals.” <a href="http://www.cs.bham.ac.uk/~jxb/INC/l16.pdf" class="uri">http://www.cs.bham.ac.uk/~jxb/INC/l16.pdf</a>.</p>
</div>
<div id="ref-shane">
<p>Lynn, Shane. 2014. “Self-Organising Maps for Customer Segmentation Using R.” January. <a href="https://www.slideshare.net/shanelynn/2014-0117-dublin-r-selforganising-maps-for-customer-segmentation-shane-lynn" class="uri">https://www.slideshare.net/shanelynn/2014-0117-dublin-r-selforganising-maps-for-customer-segmentation-shane-lynn</a>.</p>
</div>
<div id="ref-R-kohonen">
<p>Ron Wehrens, Johannes Kruisselbrink. 2017. <em>Supervised and Unsupervised Self-Organising Maps</em>. <a href="https://cran.r-project.org/web/packages/kohonen/" class="uri">https://cran.r-project.org/web/packages/kohonen/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="canonical-correlation-analysis-cca.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/generaviles/datamining.github.io/edit/master/03-som.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
