<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Syllabus for the Datamining Class</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Syllabus for the Datamining Class" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Syllabus for the Datamining Class" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Gener AvilÃ©s R">


<meta name="date" content="2017-03-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="canonical-correlation-analysis-cca.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Mining Syllabus</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-does-pca-do"><i class="fa fa-check"></i><b>2.1</b> What does PCA do?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#pca-step-by-step"><i class="fa fa-check"></i><b>2.2</b> PCA Step by Step</a><ul>
<li class="chapter" data-level="2.2.1" data-path="intro.html"><a href="intro.html#getting-the-dataset-and-things-ready."><i class="fa fa-check"></i><b>2.2.1</b> 1. Getting the dataset and things ready.</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro.html"><a href="intro.html#centering-the-points"><i class="fa fa-check"></i><b>2.2.2</b> 2. Centering the points</a></li>
<li class="chapter" data-level="2.2.3" data-path="intro.html"><a href="intro.html#compute-covariance-sigma_xy-matrix"><i class="fa fa-check"></i><b>2.2.3</b> 3. Compute covariance (<span class="math inline">\(\sigma_{X,Y}\)</span>) matrix</a></li>
<li class="chapter" data-level="2.2.4" data-path="intro.html"><a href="intro.html#eigenvectors-eigenvalues"><i class="fa fa-check"></i><b>2.2.4</b> 4. Eigenvectors + Eigenvalues</a></li>
<li class="chapter" data-level="2.2.5" data-path="intro.html"><a href="intro.html#pick-md-eigenvectors-with-highest-eigenvalues."><i class="fa fa-check"></i><b>2.2.5</b> 5. Pick <span class="math inline">\(m&lt;d\)</span> eigenvectors with highest eigenvalues.</a></li>
<li class="chapter" data-level="2.2.6" data-path="intro.html"><a href="intro.html#project-datapoints-to-those-eigenvectors."><i class="fa fa-check"></i><b>2.2.6</b> 6. Project datapoints to those eigenvectors.</a></li>
<li class="chapter" data-level="2.2.7" data-path="intro.html"><a href="intro.html#perform-analysis-as-needed-according-to-study."><i class="fa fa-check"></i><b>2.2.7</b> 7. Perform analysis as needed according to study.</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#pros-and-cons-of-pca"><i class="fa fa-check"></i><b>2.3</b> Pros and Cons of PCA</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>3</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#what-is-cca"><i class="fa fa-check"></i><b>3.1</b> What is CCA?</a></li>
<li class="chapter" data-level="3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#assumptions-for-cca"><i class="fa fa-check"></i><b>3.2</b> Assumptions for CCA</a></li>
<li class="chapter" data-level="3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#objectives-of-cca"><i class="fa fa-check"></i><b>3.3</b> Objectives of CCA</a></li>
<li class="chapter" data-level="3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#terms-used-in-the-context-of-a-cca-analysis"><i class="fa fa-check"></i><b>3.4</b> Terms used in the context of a CCA analysis</a></li>
<li class="chapter" data-level="3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#interpreting-canonical-variates"><i class="fa fa-check"></i><b>3.5</b> Interpreting canonical variates</a></li>
<li class="chapter" data-level="3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#considerations-when-working-with-cca"><i class="fa fa-check"></i><b>3.6</b> Considerations when working with CCA</a></li>
<li class="chapter" data-level="3.7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#limitations-of-cca"><i class="fa fa-check"></i><b>3.7</b> Limitations of CCA</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Syllabus for the Datamining Class</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Principal Components Analysis</h1>
<div id="what-does-pca-do" class="section level2">
<h2><span class="header-section-number">2.1</span> What does PCA do?</h2>
<p>This methods tries to explain the correlation structure of a set of predictor variables using a smaller set o linear combinations of these variables called <strong><em>components</em></strong>, note that <em>components</em> are not variables, rather indicators of linear combinations between variables.<br \> Given a dataset with <span class="math inline">\(m\)</span> variables a set of <span class="math inline">\(k\)</span> linear combinations can be used to represent it (meaning that <span class="math inline">\(k\)</span> contains almost as much information as the <span class="math inline">\(m\)</span> variables), also <span class="math inline">\(k&lt;&lt;m\)</span>.</p>
</div>
<div id="pca-step-by-step" class="section level2">
<h2><span class="header-section-number">2.2</span> PCA Step by Step</h2>
<div id="getting-the-dataset-and-things-ready." class="section level3">
<h3><span class="header-section-number">2.2.1</span> 1. Getting the dataset and things ready.</h3>
<p>Before starting the process of dimensionality reduction one should make sure the data is standardized, this is done to avoid bised in the results by values to large or to small when compared to each other. <br \><br \></p>
</div>
<div id="centering-the-points" class="section level3">
<h3><span class="header-section-number">2.2.2</span> 2. Centering the points</h3>
<ul>
<li>The <strong>standardization process</strong> is acomplished when the mean for each variable <span class="math inline">\(=0\)</span> and the standard deviation <span class="math inline">\(=1\)</span>. The following formula can be followed to acomplish this process: <span class="math display">\[Z_i = \frac {(X_i-\mu_i)}{\sigma_{ii}}\]</span></li>
</ul>
<p>Where: <span class="math inline">\(\mu_i\)</span> equals the mean of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\sigma_{ii}\)</span> equals the standard deviation of <span class="math inline">\(X_i\)</span>. <br \><br \></p>
<ul>
<li>If the values are given as a set of points the process can be acomplished with the following formula:</li>
</ul>
<p><span class="math display">\[x_{i,a} = x_{i,a} - \mu_a\]</span></p>
<p>This move will facilitate the calculations down the road.</p>
</div>
<div id="compute-covariance-sigma_xy-matrix" class="section level3">
<h3><span class="header-section-number">2.2.3</span> 3. Compute covariance (<span class="math inline">\(\sigma_{X,Y}\)</span>) matrix</h3>
<p>The <strong>covariance</strong> is a measure of the degree to which two variables vary together. Positive covariance indicates that when one variable increases, the other tends to increase. Negative covariance indicates that when one variable increases, the other tends to decrease. The covariance measure <strong>is not scaled</strong>.</p>
<p>In a <span class="math inline">\(2x2\)</span> matrix: <span class="math display">\[\begin{vmatrix}
2.0 &amp; 0.8 \\
0.8 &amp; 0.6
\end{vmatrix}\]</span></p>
<p>Since the mean (<span class="math inline">\(\mu\)</span>) is equal to <span class="math inline">\(\emptyset\)</span> thanks to <em>centering</em> the values in the previous step, the formula to calculate the covariance of the values in the matrix is: <span class="math display">\[cov(x_1,x_2) = \frac{1}{n}\sum_{i=1}^{n}x_{i,j}x_{i,2}\]</span> <strong>The way to interpret <em>covariance</em> is to understand itâs results as information about how one attribute changes as the other one changes.</strong></p>
<p>It is important to remember that, if we multiply a vector by the covariance matrix or <span class="math inline">\(\sum\)</span> the resulting vector will turn towards the direction of the variance.</p>
<p>Changing the units of measure would change the results, this is an inconvenience and is addressed by calculating the <strong><em>correlation coefficient <span class="math inline">\(r_{ij}\)</span></em></strong>:</p>
<p><span class="math inline">\(r_{ij}\)</span> scales the covariance by each of the standard deviations: <span class="math display">\[r_{ij} = \frac{\sigma_{ij}^2}{\sigma_{ii} \sigma_{jj}}\]</span> <strong>The <span class="math inline">\(r_{ij}\)</span> gives us a value with reference to know how much of a correlation exists between two variables.</strong></p>
</div>
<div id="eigenvectors-eigenvalues" class="section level3">
<h3><span class="header-section-number">2.2.4</span> 4. Eigenvectors + Eigenvalues</h3>
<p>Define a <strong>new set of dimentions</strong> by:</p>
<ol style="list-style-type: decimal">
<li>Taking the dataset and looking for the direction of the data, looking to draw a line in which, along it, there is the <strong>greatest amount of variance <span class="math inline">\(\sigma^2\)</span></strong> in the data, this line will be called the <strong>principal component 1 (PC1)</strong>.</li>
</ol>
<p><span class="math display">\[\sigma^2 = \frac{\sum(X-\mu)^2}{N}\space \space  \text{or}\space \space \sigma^2 = \frac{\sum X^2}{N} - \mu^2\]</span> <em>In the previous formula <span class="math inline">\(\sigma^2\)</span> is defined as the sum of the squared distances of each term in the distribution from the mean (<span class="math inline">\(\mu^2\)</span>) divided by the number of terms in the distribution (<span class="math inline">\(N\)</span>). In simple words: <span class="math inline">\(\sigma^2\)</span> measures <strong>how far a set of random numbers are spread out from their mean</strong>.</em></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Once PC1 is determined, it will established the next dimension by drawing an <strong><em>orthogonal</em></strong> (perpendicular) line in relation to PC1, the exact area where the line will be drawn is determined by the same process of finding the gratest <span class="math inline">\(\sigma^2\)</span> of the remaining data, once this is done PC2 is ready.</p></li>
<li><p>This will be done iteratively until all the dimensions (<span class="math inline">\(d\)</span>) of the dataset are covered and components (<span class="math inline">\(m\)</span>) are generated for every single <span class="math inline">\(d\)</span>.</p></li>
</ol>
<ul>
<li>The first <span class="math inline">\(m&lt;&lt;d\)</span> components become <span class="math inline">\(m\)</span> new dimensions.
<ul>
<li>Coordinates from every datapoint will be changed to these ânewâ dimensions.</li>
</ul></li>
<li><strong>Greatest variability</strong> is pursued to maintain the <a href="https://rpubs.com/generaviles/248692"><em>smoothness</em></a> assumption of dimensions.</li>
</ul>
<p><br \> Eigenvectors and eigenvalues are mathematically expressed as:</p>
<p><span class="math display">\[A \overrightarrow{v} = \lambda \overrightarrow{v}\]</span> Where <span class="math inline">\(A\)</span> represents <em>transformation</em>, <span class="math inline">\(\overrightarrow{v}\)</span>, a vector, also known as <strong>eigenvector</strong>, that comes out of the matrix being analysied and <span class="math inline">\(\lambda\)</span>, a scalar value also known as <strong>eigenvalue</strong>.</p>
<p><strong>Principal components = eigenvectors with largest eigenvalues.</strong><br \><br \></p>
<div id="finding-eigenvalues-and-eigenvectors" class="section level4">
<h4><span class="header-section-number">2.2.4.1</span> Finding Eigenvalues and Eigenvectors</h4>
<p>In order to exemplify the process of finding these values and vector steps are presented for a <span class="math inline">\(2x2\)</span> matrix, but this can be done with any matrix of <span class="math inline">\(n\space x\space n\)</span> dimensions following the rules of matrix algebra.</p>
<p>To begin with the example we will declare a matrix: <span class="math display">\[A =  \left[ \begin{array}{ccc}
7 &amp; 3 \\
3 &amp; -1  \end{array} \right]\]</span></p>
<p>Now the steps:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Multiply an <span class="math inline">\(n\space x\space n\)</span> identity matrix by the scalar <span class="math inline">\(\lambda\)</span>: <span class="math inline">\(IA\lambda\)</span></strong><br \> <span class="math display">\[\left[ \begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1  \end{array} \right] * \lambda = \left[ \begin{array}{cc}
\lambda &amp; 0 \\
0 &amp; \lambda  \end{array} \right]\]</span></p></li>
<li><p><strong>Substract the identity matrix multiple from matrix A: <span class="math inline">\(A-\lambda I\)</span></strong><br \> <span class="math display">\[\left[ \begin{array}{cc}
7 &amp; 3 \\
3 &amp; -1  \end{array} \right] - \left[ \begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1  \end{array} \right] = \left[ \begin{array}{cc}
7-\lambda &amp; 3 \\
3 &amp; -1-\lambda  \end{array} \right]\]</span></p></li>
<li><p><strong>Find the determinant of the matrix obtained in previous step: <span class="math inline">\(det(A-\lambda I)\)</span></strong><br \> <span class="math display">\[ det\left[ \begin{array}{cc}
7-\lambda &amp; 3 \\
3 &amp; -1-\lambda  \end{array} \right] = (7-\lambda)(-1-\lambda)-(3*3)\]</span> <span class="math display">\[= - 7 - 7 \lambda + \lambda + \lambda^2 = -16-6\lambda + \lambda^2\]</span> <span class="math display">\[= \lambda^2 - 6\lambda -16\]</span></p></li>
<li><p><strong>Solve for the values of <span class="math inline">\(\lambda\)</span> that satisfy the equation <span class="math inline">\(det(A-\lambda I)=0\)</span></strong> <br \> Solving for <span class="math inline">\(\lambda^2 - 6\lambda -16 = 0\)</span> will result in: <span class="math display">\[(\lambda-8)(\lambda+2)=0\]</span> <br \> Therfore <span class="math inline">\(\lambda_1 = 8\)</span> and <span class="math inline">\(\lambda_2 = -2\)</span> <strong>these are the eigenvalues for matrix <span class="math inline">\(A\)</span>.</strong> <br \></p></li>
<li><p><strong>Solve for the corresponding vector to each <span class="math inline">\(\lambda\)</span></strong></p></li>
</ol>
<p><strong>Solving for </strong><span class="math inline">\(\lambda = 8\)</span><strong>, in this process we will call the matrix with substituted values: <span class="math inline">\(B\)</span>.</strong><br \></p>
<p><span class="math display">\[ \left[ \begin{array}{cc}
7-(8) &amp; 3 \\
3 &amp; -1-(8)  \end{array} \right] =  \left[ \begin{array}{cc}
-1 &amp; 3 \\
3 &amp; -9  \end{array} \right]\]</span></p>
<p>We will assume the following <span class="math inline">\(B \overline X = 0 \space \therefore\)</span></p>
<p><span class="math display">\[\left[ \begin{array}{cc}
-1 &amp; 3 \\
3 &amp; -9  \end{array} \right] \left[ \begin{array}{cc}
x_1 \\
x_2 \end{array} \right] = \left[ \begin{array}{cc}
0 \\
0 \end{array} \right]\]</span></p>
<p>Applying row reduction <span class="math inline">\(3R_1 + R_2 = R_2\)</span> to: <span class="math display">\[\left[ \begin{array}{cc|r}
-1 &amp; 3 &amp; 0\\
3 &amp; -9 &amp; 0  \end{array} \right] = \left[ \begin{array}{cc|r}
-1 &amp; 3 &amp; 0\\
0 &amp; 0 &amp; 0  \end{array} \right] \space \therefore -x_1+3x_2 = 0\]</span></p>
<p>From the previous operation we obtain <span class="math inline">\(3x_2 = x_1\)</span>, at this point we can choose a value for any <span class="math inline">\(x\)</span>, we will go for <span class="math inline">\(x_2 = 1\)</span> to keep it simple.</p>
<p><span class="math display">\[3x_2 = x_1 \space \therefore 3(1) = x_1 \space \therefore \space x_1 = 3\]</span></p>
<p><strong>Now we know that the eigenvalue <span class="math inline">\(\lambda = 8\)</span> $ corresponds to the eigenvector <span class="math inline">\(\overline X = (3,1)\)</span>.</strong><br \><br \></p>
<p><strong>Solving for <span class="math inline">\(\lambda -2\)</span>, generating matrix <span class="math inline">\(C\)</span>.</strong><br \> <span class="math display">\[C = \left[ \begin{array}{cc}
7-(-2) &amp; 3 \\
3 &amp; -1-(-2)  \end{array} \right]\]</span> <span class="math inline">\(C\overline X = 0 \space \therefore\)</span></p>
<p><span class="math display">\[\left[ \begin{array}{cc}
9 &amp; 3 \\
3 &amp; 1  \end{array} \right] \left[ \begin{array}{c}
x_1 \\
x_2  \end{array} \right] = \left[ \begin{array}{c}
0 \\
0  \end{array} \right]\]</span></p>
<p>Applying row reduction <span class="math inline">\(3R_2 - R_1 = R_1\)</span>:</p>
<p><span class="math display">\[\left[ \begin{array}{cc|r}
0 &amp; 0 &amp; 0\\
3 &amp; 1 &amp; 0 \end{array} \right] \space \therefore 3x_1 + x_2 = 0\]</span></p>
<p>Assigning <span class="math inline">\(x_1 = 1\)</span>: <span class="math display">\[x_2 = -3x_1 \space \therefore x_2 = -3(1)\]</span></p>
<p><strong>The eigenvalue <span class="math inline">\(\lambda = 8\)</span> corresponds to the eigenvector <span class="math inline">\(\overline X = (1,-3)\)</span></strong><br \><br \></p>
</div>
</div>
<div id="pick-md-eigenvectors-with-highest-eigenvalues." class="section level3">
<h3><span class="header-section-number">2.2.5</span> 5. Pick <span class="math inline">\(m&lt;d\)</span> eigenvectors with highest eigenvalues.</h3>
<p>In other words, usually the <strong>2</strong> eigenvectors with the highest scalars, or <span class="math inline">\(\lambda\)</span>, will be selected to represent the whole dataset as <em>Principal Component 1</em> and <em>Principal Component 2</em>.</p>
</div>
<div id="project-datapoints-to-those-eigenvectors." class="section level3">
<h3><span class="header-section-number">2.2.6</span> 6. Project datapoints to those eigenvectors.</h3>
<p>One or the algoritm has to project the datapoints to these new set of dimensions so they can be analyized.</p>
</div>
<div id="perform-analysis-as-needed-according-to-study." class="section level3">
<h3><span class="header-section-number">2.2.7</span> 7. Perform analysis as needed according to study.</h3>
</div>
</div>
<div id="pros-and-cons-of-pca" class="section level2">
<h2><span class="header-section-number">2.3</span> Pros and Cons of PCA</h2>
<p>This algorithm, as all, is better suited for specific circumstances and performs poorly in others. The following list trys to summarize this idea:</p>
<div id="pros" class="section level4">
<h4><span class="header-section-number">2.3.0.1</span> <strong>Pros</strong></h4>
<ul>
<li>Reduction in size of data.</li>
<li>Allows estimation of probabilites in high-dimensional data.</li>
<li>It renders a set of components that are uncorrelated.</li>
</ul>
</div>
<div id="cons" class="section level4">
<h4><span class="header-section-number">2.3.0.2</span> <strong>Cons</strong></h4>
<ul>
<li>It has a high computational cost, therefore it cannot be applied to very large datasets.</li>
<li>Not good when working with fine-grained classes.</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="canonical-correlation-analysis-cca.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/generaviles/datamining.github.io/edit/master/01-pca.Rmd",
"text": "Edit"
},
"download": ["bookdown-demo.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
